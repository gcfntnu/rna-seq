#-*- mode: snakemake -*-
"""Workflow for analysing the unmapped reads form genome alignment.

Loosely based on the Read Origin Protocol https://github.com/smangul1/rop

Main steps
----------

1.) Quality filter
2.) Lost human reads
2.) RRNA 
3.) repeats
4.) Lymphocyte reportaire
5.) Microbiome


The rules from star.rules is pretty much copied over and prefixed with ta_
The output will be an updated gtf file.

FIXME: bug in bowtie2 gave random errors. Added --random to supress
https://github.com/BenLangmead/bowtie2/issues/141

"""

import os
from os.path import join

rule unaligned_bam_se:
    input:
        '{sample}.bam'
    output:
        '{sample}.unmapped.fastq'
    shell:
        'samtools view '
        '-f 0x4 '
        '-bh  '
        '{input} '
        '| samtools bam2fq - '
        '> {output} '
        
rule unaligned_bam_pe:
    input:
        '{sample}.bam'
    output:
        r1 = '{sample}.unmapped_R1.fastq',
        r2 = '{sample}.unmapped_R2.fastq'
    shell:
        """
        samtools view -uf64 {input} |samtools bam2fq - | gzip >{output.r1}
        samtools view -uf128 {input} |samtools bam2fq - |gzip >{output.r2}
        """

rule fqtrim_pe:
    input:
        r1 = join(config['tmp_dir'], 'star', '{sample}.Unmapped.out.mate1'),
        r2 = join(config['tmp_dir'], 'star', '{sample}.Unmapped.out.mate2')
    output:
        r1 = 'trimmed/{sample}_R1.trimmed.fastq',
        r2 = 'trimmed/{sample}_R2.trimmed.fastq'
    params:
        adapter= config.get('adapter', '')
    log:
        'logs/{sample}.trim.log'
    threads:
        4
    shell:
        'fqtrim '
        '-q 10 '
        '-l 22 '
        '-o trimmed.fq '
        '-T '
        '-D '
        '-V '
        '-p {threads} '
        '{input.r1},{input.r2} '
        '2> {log} '
        '&& '
        'mv {wildcards.sample}.Unmapped.out.mate1.trimmed.fq {output.r1} '
        '&& mv {wildcards.sample}.Unmapped.out.mate2.trimmed.fq {output.r2} '

rule fqtrim_se:
    input:
        r1 = join(config['tmp_dir'], 'star', '{sample}.Unmapped.out.mate1')
    output:
        r1 = 'trimmed/{sample}.trimmed.fastq'
    params:
        adapter= ''
    threads:
        4
    log:
        'logs/{sample}.trim.log'
    shell:
        'fqtrim '
        '-q 10 '
        '-l 22 '
        '-o trimmed.fq '
        '-T '
        '-D '
        '-V '
        '-p {threads} '
        '-P33 '
        '{input.r1} '
        '2> {log} '
        '&& '
        'mv {wildcards.sample}.Unmapped.out.mate1.trimmed.fq {output.r1} '
        
DB = 'silva-arc-16s-id95 silva-arc-23s-id98 silva-bac-16s-id90 silva-bac-23s-id98 silva-euk-18s-id95 silva-euk-28s-id98 rfam-5.8s-database-id98 rfam-5s-database-id98'.split()
##DB = 'silva-euk-18s-id95 silva-euk-28s-id98'.split()

rule align_ribosome:
    input:
        fastq = 'trimmed/{sample}.trimmed.fastq',
        ref = expand('/ext_db/homo_sapiens/GRCh38/sortmerna_db/{db}.idx.stats', db=DB)
    params:
        ref = ':'.join(expand('/ext_db/homo_sapiens/GRCh38/sortmerna_db/{db}.fasta,/ext_db/homo_sapiens/GRCh38/sortmerna_db/{db}.idx', db=DB)),
        clean_fastq = 'ribo_clean/{sample}',
        ribo_fastq = 'ribo_clean/{sample}_ribo'
    output:
        clean_fastq = 'ribo_clean/{sample}.fastq',
        ribo_fastq = 'ribo_clean/{sample}_ribo.fastq'
    threads:
        4
    benchmark:
        'timings/sorterma.txt'
    log:
        'logs/{sample}-sortmerna.log'
    shell:
        'sortmerna '
        '--ref {params.ref} '
        '--reads {input.fastq} '
        '--fastx '
        '--aligned {params.ribo_fastq} '
        '--other {params.clean_fastq} '
        '-a {threads} '
        '--log TRUE '
        '&& mv {params.ribo_fastq}.log  {log} '
        
rule align_genome_se:
    input:
       'ribo_clean/{sample}.fastq'
    output:
        clean_fastq = 'aligned/_{sample}.fq',
        genome_fastq = 'aligned/{sample}_genome.fastq',
        sam = 'aligned/{sample}_genome.sam'
    params:
        genome = '/ext_db/homo_sapiens/GRCh38/indices/bowtie2/genome'
    threads:
        16
    log:
        'logs/{sample}-bowtie-genome.log'
    shell:
        'bowtie2 '
        '--very-sensitive-local '
        '--phred33 '
        '-p {threads} '
        '--un {output.clean_fastq} '
        '--al {output.genome_fastq} '
        '-x {params.genome} '
        '--reorder '
        '{input} '
        '-S {output.sam} '
        '> {log} '
        
rule align_transcriptome_se:
    input:
       'aligned/_{sample}.fq'
    output:
        clean_fastq = 'aligned/{sample}.fastq',
        tx_fastq = 'aligned/{sample}_tx.fq',
        sam = 'aligned/{sample}_genome.sam'
    params:
        tx = '/ext_db/homo_sapiens/GRCh38/indices/bowtie2/transcriptome'
    threads:
        16
    log:
        'logs/{sample}-bowtie-transcriptome.log'
    shell:
        'bowtie2 '
        '--very-sensitive-local '
        '--phred33 '
        '-p {threads} '
        '--un {output.clean_fastq} '
        '--al {output.tx_fastq} '
        '-x {params.tx} '
        '--reorder '
        '{input} '
        '-S {output.sam} '
        '> {log} '
        
rule align_transcriptome:
    input:
        'aligned/genome.fastq'
    output:
        clean_fastq = 'aligned/{sample}.fastq',
        tx_fastq = 'aligned/{sample}_transcriptome.fastq'


rule aligned_fasta:
    input:
        'aligned/{sample}.fastq'
    output:
        temp('aligned/{sample}.fa')
    shell:
        'fastq_to_fasta -i {input} -o {output}'
        
rule align_repeats:
    input:
       fastq = 'aligned/{sample}.fa',
       index = 'repeats/db/repbase.fa'
    output:
        'repeats/{sample}.sam'
    log:
        'logs/repats-blastn.log'
    shell:
        'blastn -task megablast '
        '-index_name {input.index} '
        '-use_index true '
        '-query {input.fastq} '
        '-db {input.index} '
        '-dust no '
        '-outfmt 17 '
        '-evalue 10e-5 '
        '-perc_identity 95 '
        '> {output} '
        '2> {log} '

rule extract_repeats_fastq:
    input:
        sam = 'repeats/{sample}.sam',
        fastq = 'aligned/{sample}.fastq'
    output:
        clean_fastq = 'repeats/{sample}.fastq',
        rep_fastq = 'repeats/{sample}_repeats.fastq'
    run:
        import dinopy
        
        repeat_names = set()
        with open(input.sam) as fh:
            for line in fh.read().splitlines():
                name = line.split('\t')[0]
                repeat_names.add(name)
        repeats, clean = [],[]
        import dinopy
        fqr = dinopy.FastqReader(input.fastq)
        for sequence, name, qvals in fqr.reads(quality_values=True):
            if name.decode('utf-8') in repeat_names:
                repeats.append((sequence, name, qvals))
            else:
                clean.append((sequence, name, qvals))
        with dinopy.FastqWriter(output.clean_fastq) as c_faw:
            c_faw.write_reads(clean)
        with dinopy.FastqWriter(output.rep_fastq) as r_faw:
            r_faw.write_reads(repeats)

rule mixcr_align:
    input:
        'repeats/{sample}.fastq'
    output:
        vdjca = temp('{sample}_alignments.vdjca')
    threads:
        16
    shell:
        """
        mixcr align -p rna-seq -g -s hsa --save-description -OallowPartialAlignments=true {input} {output.vdjca}
        """
        
rule mixcr_partial_assembly1:
    input:
        rules.mixcr_align.output.vdjca
    output:
        temp('{sample}_alignments_rescued.vdjca')
    shell:
        'mixcr assemblePartial {input} {output} '
        
rule mixcr_partial_assembly2:
    input:
        rules.mixcr_partial_assembly1.output
    output:
        temp('{sample}_alignments_rescued_2.vdjca')
    shell:
        'mixcr assemblePartial -d {input} {output} '

rule mixcr_assembly:
    input:
        rules.mixcr_partial_assembly2.output
    output:
        temp('{sample}_mixcr.clns')
    log:
        'logs/{sample}-mixcr.log'
    shell:
        'mixcr assemble -r {log} {input} {output} '

rule mixcr_clones:
    input:
        rules.mixcr_assembly.output
    output:
        'immune/mixcr/{sample}.clones'
    shell:
        'mixcr exportClones {input} {output}'

rule mixcr_aligned_fastq:
    input:
        rules.mixcr_partial_assembly2.output
    output:
        'immune/mixcr/{sample}_immune.fastq'
    shell:
        'mixcr exportReads {input} {output}'
    
rule extract_mixcr_fastq:
    input:
        clones = 'immune/mixcr/{sample}.clones',
        immune_fastq = 'immune/mixcr/{sample}_immune.fastq',
        fastq = 'repeats/{sample}.fastq'
    output:
        clean_fastq = 'immune/mixcr/{sample}.fastq'
    run:
        import dinopy
        
        read_names = set()
        clean = []
        fqr0 = dinopy.FastqReader(input.immune_fastq)
        for sequence, name in fqr0.reads(quality_values=False):
            read_names.add(name)
        fqr = dinopy.FastqReader(input.fastq)
        for sequence, name, qvals in fqr.reads(quality_values=True):
            if not name in read_names:
                clean.append((sequence, name, qvals))
        with dinopy.FastqWriter(output.clean_fastq) as c_faw:
            c_faw.write_reads(clean)

rule minikraken_8gb:
    output:
        'data/ext/minikraken_8GB/database.idx'
    params:
        db = 'data/ext/minikraken_8GB',
        url = 'https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_8GB.tgz'
    shell:
        'curl {params.url} | tar xz -C {params.db} --strip-components=1'
        
rule minikraken_4gb:
    output:
        'data/ext/minikraken_4GB/database.idx'
    params:
        db = 'data/ext/minikraken_4GB',
        url = 'https://ccb.jhu.edu/software/kraken/dl/minikraken_20171019_4GB.tgz'
    shell:
        'curl {params.url} | tar xz -C {params.db} --strip-components=1'

rule kraken_map:
    input:
       fastq = 'immune/mixcr/{sample}.fastq',
       db = rules.minikraken_8gb.output if config.get('kraken_db', '8GB') == '8GB' else rules.minikraken_8gb.output
    params:
        rules.minikraken_8gb.params.db if config.get('kraken_db', '8GB') == '8GB' else rules.minikraken_8gb.params.db
    output:
        'metagenome/kraken8/{sample}.seq'
    shell:
        'kraken --db {params} {input.fastq} > {output} '

rule kraken_report:
    input:
        'metagenome/kraken8/{sample}.seq'
    output:
        'metagenome/kraken8/{sample}.report'
    params:
        rules.minikraken_8gb.params.db if config.get('kraken_db', '8GB') == '8GB' else rules.minikraken_8gb.params.db   
    shell:
        'kraken-report --db {params} {input} > {output}'
        
rule otu_table:
    input:
        expand('metagenome/kraken8/{sample}.report', sample=SAMPLES)
    output:
        'metagenome/genus_8gb.biom'
    shell:
        'kraken-biom {input} --min G --max F -o {output}'


rule heatmap_otu:
    input:
        sampleinfo = 'data/processed/sample_info.txt'
        otu = 'metagenome/genus_8gb.biom'
    shell:
        'python scripts/otu_heatmap.py {input.sampleinfo} {input.otu}'
